<!DOCTYPE html>
<html lang="en" class="light layout_html_container__LIkV6">

<head>
    <meta charSet="utf-8" />
    <link rel="stylesheet" href="/_next/static/css/eee66c5eb8dff5dd.css" data-precedence="next.js" />
    <link rel="stylesheet" href="/_next/static/css/86333f61218991e5.css" data-precedence="next.js" />
    <link rel="stylesheet" href="/_next/static/css/08489188df8cd5b7.css" data-precedence="next.js" />
    <link rel="stylesheet" href="/_next/static/css/a8a68dea0a60e9b6.css" data-precedence="next.js" />
    <link rel="stylesheet" href="/_next/static/css/09ff7ae0c74d1ead.css" data-precedence="next.js" />
    <link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" />
    <title>Atomic</title>
    <meta name="description" content="Internet-scale machine learning" />
    <meta property="og:title" content="Atomic" />
    <meta property="og:description" content="Internet-scale machine learning" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="/_next/static/media/metadata/favicon.83662ded.ico" type="image/x-icon" sizes="any" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" />
    <script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" nomodule=""></script>
    <style>
        .page_section__q5fw_, .page_title_section__71Edx {
    align-items: center;
    display: flex;
    flex-direction: column;
    justify-content: center;
}
        .page_page_container__7x71k {
    align-items: center;
    display: flex;
    flex-direction: column;
    font-family: Haffer;
    font-size: 12px;
    font-weight: 400;
    gap: 105px;
    justify-content: center;
    letter-spacing: 3%;
    line-height: 180%;
    padding: 100px 32px;
    text-align: justify;
    width: -webkit-fill-available;
    width: -moz-available;
    width: stretch;
}
.page_image_container__a9b9q {
    align-items: center;
    color: #000;
    display: flex;
    flex-direction: column;
    font-size: 12px;
    gap: 32px;
    height: 100%;
    justify-content: center;
    text-align: center;
    width: 100%;
}
.page_paper_title__XlBTD {
    font-size: 20px;
}
.page_subtitle__yxpgk {
    font-family: FiraCode;
    font-size: 12px;
    font-weight: 200;
    padding-bottom: 33px;
    text-transform: uppercase;
}
.page_abstract_text__faafI {
    font-size: 12px;
    text-align: center;
}
.page_section__q5fw_ {
    gap: 52px;
    max-width: 665px;
}
    </style>
</head>

<body class="layout_body_container__q8Oiv layout_body_container__q8Oiv">
    <main class="layout_main_container__1pGaq">
        <div class="Header_header_wrapper__IEkIo">
            <div class="Header_container__R0wlW"><a class="Header_tao_logo__70xe7" href="/"><svg width="21"
                        height="23" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path
                            d="M12.53 17.783v-9.08a4.144 4.144 0 0 0-4.14-4.117v14.511a3.8 3.8 0 0 0 3.96 3.841 4.275 4.275 0 0 0 2.816-.816c-2.39-.253-2.635-1.693-2.635-4.339Z"
                            fill="currentColor"></path>
                        <path d="M3.775.787A3.8 3.8 0 0 0 0 4.587h16.893a3.8 3.8 0 0 0 3.775-3.8H3.775Z"
                            fill="currentColor"></path>
                    </svg></a>
                <div class="Header_menu_content__WPcNO" style="width:100%">
                    <nav class="Header_nav_wrapper__02Cvo">
                        <ul class="Header_menu_list__V3odZ">
                            <!-- <li class="Header_menu_item__KzwZa"><a class="Link_link__hI8T_" target="_self"
                                    href="docslink">Docs</a></li> -->
                            <li class="Header_menu_item__KzwZa"><a class="Link_link__hI8T_" target="_self"
                                    href="about.html">About</a></li>
                            <li class="Header_menu_item__KzwZa"><a class="Link_link__hI8T_" target="_self"
                                    href="whitepaper.html">Whitepaper</a></li>
                            <li class="Header_menu_item__KzwZa"><a class="Link_link__hI8T_" target="_self"
                                    href="charter.html">Charter</a></li>
                            <li class="Header_menu_item__KzwZa"><a class="Link_link__hI8T_" target="_self"
                                    href="academia.html">Academia</a></li>
                            <li class="Header_menu_item__KzwZa"><a class="Link_link__hI8T_" target="_self"
                                    href="scanlink">Scan</a></li>
                            
                        </ul>
                    </nav>
                </div>
                <div class="Header_hamburger_container__Ns3iZ">
                    <div><button class="HamburgerMenu_hamburger_btn__neLmn"><svg width="17" height="17" fill="none"
                                xmlns="http://www.w3.org/2000/svg">
                                <path d="M1 8.862h15M1 1.93h15M1 15.793h15" stroke="currentColor" stroke-width="2"
                                    stroke-linecap="round" stroke-linejoin="round"></path>
                            </svg></button>
                        <div class="HamburgerMenu_hamburger_menu__rGm5w HamburgerMenu_hamburger_menu_close__oSteL">
                            <button class="HamburgerMenu_close_btn__9_tbo"><svg width="12" height="12" fill="none"
                                    xmlns="http://www.w3.org/2000/svg">
                                    <path
                                        d="M.397 12 0 11.603 5.603 6 0 .397.397 0 6 5.603 11.603 0 12 .397 6.397 6 12 11.603l-.397.397L6 6.397.397 12Z"
                                        fill="currentColor"></path>
                                </svg></button>
                            <div class="HamburgerMenu_menu_list__yiwEL">
                                <div class="HamburgerMenuItems_menu_wrapper__6hFz3"><a class="Link_link__hI8T_"
                                        target="_self" href="#!">
                                        <p class="HamburgerMenuItems_menu_link__UCzJc">Build</p>
                                    </a><a class="Link_link__hI8T_" target="_self" href="about.html">
                                        <p class="HamburgerMenuItems_menu_link__UCzJc">About</p>
                                    </a><a class="Link_link__hI8T_" target="_self" href="whitepaper.html">
                                        <p class="HamburgerMenuItems_menu_link__UCzJc">Whitepaper</p>
                                    </a>
                                    </a><a class="Link_link__hI8T_" target="_self" href="academia.html">
                                        <p class="HamburgerMenuItems_menu_link__UCzJc">Academia</p>
                                    </a><a class="Link_link__hI8T_" target="_blank"
                                        href="xlink">
                                        <p class="HamburgerMenuItems_menu_link__UCzJc">Telegram</p>
                                    </a><a class="Link_link__hI8T_" target="_blank"
                                        href="tglink">
                                        <p class="HamburgerMenuItems_menu_link__UCzJc">Code</p>
                                    </a></div>
                            </div>
                        </div>
                    </div>
                </div>
                <li class="Header_menu_item__KzwZa Header_menu_content__WPcNO">
                    <div class="Header_tao_credit_container__aguTx">
                        <div class="TaoCredit_tao_credit_val__3C0k7"><a class="Link_link__hI8T_ is-button"
                                target="_self"
                                href="https://atomicsor.com/wallet"><span></span><span></span></a></div>
                    </div>
                </li>
            </div>
        </div>
        <div style="max-width: 800px; width: 90%; text-align: center;; margin: 0 auto !important;" class="page_page_container__7x71k" style="opacity:0">
            <section class="page_title_section__71Edx">
                <p class="page_paper_title__XlBTD">Incentivizing Intelligence: The Atomic Approach</p>
                <p class="page_subtitle__yxpgk">Jacob Steeves / Ala Shaabana / Yuqian Hu / Francois Luus / Sin Tai Liu /
                    Jacqueline Dawn Tasker-Steeves / Opentensor Foundation</p><img alt="double tao logo" loading="lazy"
                    width="40" height="40" decoding="async" data-nimg="1" style="color:transparent"
                    src="/images/icons/double-tao-logo.svg" />
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">00/ Abstract</p>
                <p class="page_abstract_text__faafI">Inspired by the efficiency of financial markets, we propose that a
                    market system can be used to effectively produce machine intelligence. This paper introduces a
                    mechanism in which machine intelligence is valued by other intelligence systems peer-to-peer across
                    the internet. Peers rank each other by training neural networks that are able to learn the value of
                    their neighbours, while scores accumulate on a digital ledger. High-ranking peers are rewarded with
                    additional weight in the network. In addition, the network features an incentive mechanism designed
                    to resist collusion. The result is a collectively run machine intelligence market that continually
                    produces newly trained models and rewards participants who contribute information-theoretic value to
                    the system.</p>
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">01/ Introduction</p>
                <p>The production of machine intelligence relies almost exclusively on a system of benchmarking, in
                    which a model’s performance is assessed on narrowly defined supervised problems. While this method
                    works well for task-specific problems, measuring machine intelligence solely with supervised
                    objectives causes the field to converge towards narrow specialists, as opposed to resilient
                    generalists (Radford et al. [2019]).</p>
                <p>In 2012, an inflection point occurred when AlexNet achieved state-of-the-art performance on the
                    ImageNet competition (Krizhevsky et al. [2017]). This triggered a sharp acceleration in the
                    development of new neural network approaches able to overcome purely statistical methods with high
                    dimensional, representation-based approaches. Only a few years later, Vaswani et al. introduced the
                    first-ever unsupervised learning method that would surpass supervised learning regimes in language
                    understanding (Vaswani et al. [2017]). These innovations demonstrate that researchers can achieve
                    state of the art results by making consistent, incremental improvements upon previous work,
                    consistently adding to a shared base of research over time.</p>
                <p>However, since intelligence produced by these models is always lost, this approach is quite
                    inefficient. Users have to retrain models on their own systems to replicate, or improve upon, the
                    work of others. Consequently, this leads to unnecessary computational loss in learning tasks that
                    other models have already learned.</p>
                <p>Additionally, there is the issue of model evaluation. Presently, new research must go through the
                    academic peer review process to investigate the work while ensuring quality and correctness. While
                    this process has its benefits, it creates a system that is unscalable and subject to human bias. It
                    is notoriously difficult to reproduce artificial intelligence research Gundersen and Kjensmo [2018].
                    There must be a more efficient and objective method to evaluate AI model performance.</p>
                <p>In this paper, we introduce the Atomic Network: a decentralized peer-to-peer machine learning
                    protocol. In the network, machine intelligence is measured by other intelligence systems in a
                    continuous and asynchronous peer-to-peer (P2P) fashion across the internet. Models are ranked for
                    informational production regardless of the subjective task or dataset used to train them. By
                    changing the basis against which machine intelligence is measured, the market can reward
                    intelligence that is applicable to a much larger set of objectives while still preserving the unique
                    value of legacy systems and smaller diverse systems that find niches within the reward landscape.
                    The constructed market uses a digital ledger to record ranks and to provide incentives to the
                    researchers, thus allowing them to directly monetize their machine intelligence work. It is divided
                    into two layers: the AI layer and the Blockchain layer (Figure 1).</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_1_1.png"
                        alt="schematic of the Atomic network" class="page_image_container_image__b_Abq" />
                    <p><span class="page_image_container_caption_no__RyBQf">Figure 1 / </span>schematic of the Atomic
                        network.</p>
                </div>
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">02 Blockchain layer</p>
                <p>The Blockchain layer is a layer-0 blockchain based on Polkadot Substrate (Wood [2016]), and is
                    responsible for enforcing the consensus mechanism, ensuring peer identity, and incentivizing network
                    peers. The blockchain layer resides directly underneath the AI layer, and communication between the
                    two layers is achieved through inter-process communication. In order to fairly distribute incentive
                    across all of the participating peers, the Atomic network leverages a staked weighted trust
                    through consensus. Peers are incentivized to rank each other, and highly ranked peers will receive
                    additional rewards. The blockchain does not trust rankings from any individual peer on the network,
                    but rather trusts the collective rankings from all of the participating peers. In order to submit
                    their rankings, peers must first register their wallets, and a pair of cryptographic keys that are
                    unique to each peer. The same keys will be required to sign all transactions to the chain and all
                    communication requests between peers. These cryptographic keys will serve as the main identification
                    tool for peers on the network.</p>
                <p>Additionally, the blockchain maintains the system’s incentive mechanism, designed primarily for
                    peer-rankings systems and to solve for the problem of collusion, which occurs when a small fraction
                    of the network works selfishly together as a means to maximize their rewards.</p>
                <p>To solve this problem, we have leveraged a trust based incentive mechanism by rewarding peers in the
                    network who are “trusted” and have reached consensus. The incentive mechanism utilizes a stake
                    vector S and a set of weights W where rows are inter-peer rankings. We also infer a trust matrix T
                    from the weights, where ti,j = 1 if and only if there is a non-zero edge between peer i and j.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_2_1.png" alt="Trust matrix"
                        class="page_image_container_image__b_Abq" /></div>
                <p>We define peers who have reached “consensus” as those with a positive weight settings from more than
                    50 percent of stake (T &gt; 0.5) in the network. Hence, when more than 50% of the incentive goes to
                    consensus peers, then the chain can be considered as having reached consensus. This is regularized
                    on the chain by the sigmoid function defined by Equation 2 such that peers who have reached
                    consensus receive significantly higher rewards. Effectively, the threshold-like sigmoid rewards
                    connected peers and punishes the untrustworthy peers.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_2_2.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" /></div>
                <p>We use the consensus term to scale the original rankings. This ensures the majority of the incentive
                    is distributed to peers that trusted by a majority of the network.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_2_3.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" /></div>
                <p>To give an example of how this might work in practice, consider a scenario in which a colluding cabal
                    C has formed on the network Figure 4. The cabal will actively try to maximize their inflation by
                    only ranking themselves highly while providing no informational knowledge for the honest H peers on
                    the network. In this disconnected network, the two subgroups will only vote for peers in its own
                    group with little to no weights for peers of the opposite group (Figure 4). Assuming that the chain
                    has reached consensus, the fight between honest H peers and cabal C peers can be determined by the
                    portion of initial stake each sub-group contains.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_2_4.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" />
                    <p><span class="page_image_container_caption_no__RyBQf">Figure 2 / </span>Disjointed sub-networks:
                        Cabal peers (left) only vote within its own group and does not interact with a majority of the
                        peers and stake on the network. This results a lower consensus C score for the sub-network and
                        thus a lower incentive.</p>
                </div>
                <p>In the event that the stake in the honest sub-graph is greater than the cabal (SH &gt; SC ), only
                    peers within the honest subgroup are capable of reaching consensus and will receive the majority of
                    the incentive from the chain. The disagreement between the majority stake holders SH, and the
                    dishonest sub-graph results in a penalized consensus and therefore incentive for the C sub-graph. As
                    such, the ratio of stake SC SC +SH decreases over time, and the cabal must continually add stake to
                    maintain its position. These cabal attacks are akin to the 51% attacks present in other blockchains
                    (Sayeed and Marco-Gisbert [2019]). However, our stake based ranking system – like other proof of
                    stake systems – present significantly higher risks for the attackers that endanger the value of
                    their own tokens.</p>
                <p>Additionally, our incentive mechanism includes a bonding mechanism (Appendix Section 9) to provide
                    additional incentives for peers who validate honestly. Similar to market-based speculation in
                    traditional equities, the peers that have accumulated bonds in peers that others will later value
                    attain increased inflation themselves.</p>
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">03 AI layer</p>
                <p>In addition to inference and training, the AI layer is responsible for abstracting away the Atomic
                    kernel and ensuring input/output compatibility of a node’s neural network with the rest of the
                    network’s peers. Each node on the Atomic Protocol contains a single neural network. From now on,
                    we will be referring to these nodes as neurons; this is not to be confused with individual neurons
                    in traditional neural networks.</p>
                <p>The Atomic Protocol is composed of n parameterized functions (or neurons) F = f0, ..., fj , ...fn
                    distributed in a peer-to-peer fashion. Each neuron is holding zero or more network weights S = [si ]
                    (or “stake”) represented on a digital ledger. Our goal is the distribution of stake I – as incentive
                    – to peers who have contributed the most informational value.</p>
                <p>Figure 2 shows a high level overview of the forward and backward passes performed on the Atomic
                    network and describes the process by which neurons train on the Atomic protocol. Similarly to the
                    Hivemind proposal by Ryabinin et al., peers are able to request others for forward passes and
                    subsequently propagate backward gradients over the wire. By querying for information, Atomic
                    offers inference across all of the nodes serving on the network. Unique to Atomic, this enables
                    an distributed trust-less style of machine learning where requests are authenticated and verified
                    using the digital ledger. The Blockchain layer ensures the identity of each peer through a unique
                    cryptographic key and maintains the integrity of incentive distribution and rankings over the entire
                    network.</p>
                <p>In Atomic, peers who are highly ranked receive the most incentive. Peers use the outputs of others
                    F(x) = [f0(x)...fn(x)] as inputs to themselves f(F(x)) and learn a set of weights W = [wi,j ] where
                    peer i is responsible for setting the i th row through transactions on a digital ledger. We describe
                    peer ranking in more detail in Section 4.</p>
                <p>Figure 3 shows a cutaway diagram of two neurons on the Atomic network. Synapses function as the
                    main transmitter between neurons and ensure all of the transmission tensors are formatted in
                    correspondence to the Atomic protocol. Each synapse dictates a different ML task, and therefore
                    follows a different compression and communication process. This ensures that all logits and
                    embeddings being transported over the Atomic protocol are sanitized, correct, and contribute
                    useful information. If a neuron is acting maliciously or sending incorrect tensors across the wire,
                    Synapses ensure that their rankings and rewards are penalized as a result. Currently, the Atomic
                    network is focused on large language modelling and contains 4 separate synapses for text-related
                    tasks ( LastHiddenState, CausalLM, CausalLMNext, and Seq2Seq). Prior works have shown that large
                    language models can be continually fine-tuned for improved performance for downstream tasks (Scialom
                    et al. [2022],Ouyang et al. [2022]). The Atomic protocol achieves the same continual fine-tuning
                    process while being fully distributed and trust-less over the web.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_3_1.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" />
                    <p><span class="page_image_container_caption_no__RyBQf">Figure 3 / </span>Neurons training on the
                        Atomic Protocol. The Atomic protocol utilizes a mixture-ofexperts (MoE) architecture with
                        each remote peer acting as a single expert(Shazeer et al. [2017]; Ryabinin and Gusev [2020]).
                    </p>
                </div>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_3_2.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" />
                    <p><span class="page_image_container_caption_no__RyBQf">Figure 4 / </span>Neurons training on the
                        Atomic Protocol. The Atomic protocol utilizes a mixture-ofexperts (MoE) architecture with
                        each remote peer acting as a single expert(Shazeer et al. [2017]; Ryabinin and Gusev [2020]).
                    </p>
                </div>
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">04 game-theoretic scoring</p>
                <p>A crucial element of the Atomic Protocol is the production of an accurate ranking method. Peers
                    are ranked based on their informational significance to the overall collective, represented by a
                    ranking r = [ri ] where score ri ∈ R represents a participant’s contribution to the benchmark. This
                    is done by calculating its Shapley value ϕ (Shapley [1952]) which is a way to fairly distribute the
                    contribution for each participant in a group by permuting through all possible combinations of the
                    subsets:</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_4_1.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" /></div>
                <p>where N and S represent the complete set and a single set of peers respectively. CE(S) is the
                    cross-entropy loss give S set of peers, ϕi represents the Shapley value for a peer i.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_4_2.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" />
                    <p><span class="page_image_container_caption_no__RyBQf">Figure 5 / </span>Shapley score comparison
                        between large language models (LLM) of various sizes (GPT 2 — 117M, GPT-neo — 125M, GPT-J — 6B,
                        GPT-neox — 30B) (Wang and Komatsuzaki [2021] , Black et al. [2022]) by a single validator. The
                        scoring was done on the Atomic network, and models were scored based on their performance at
                        last token prediction. Larger models capable of achieving lower loss are given a higher Shapley
                        score by their peer.</p>
                </div>
                <p>In order to calculate the Shapley score for thousands of peers, the network leverages a sparse Monte
                    Carlo estimation method. In terms of the AI layer, this is akin to a sparsely gated
                    mixture-of-experts (MoE) layer (Shazeer et al. [2017], Fedus et al. [2021]) such that only a portion
                    of the network is queried at a given time. This was implemented to allow the network to scale
                    efficiently without facing any computational bottlenecks.</p>
                <div class="page_image_container__a9b9q"><img src="/images/academia/section_4_3.png"
                        alt="consensus equation" class="page_image_container_image__b_Abq" /></div>
                <p>where gj (x) represents the output of the gating network and fj (x) represents the response from
                    peer, j. Unlike previous MoE models where routing determines the parameters to activate, our gating
                    layer determines the optimal peers in the network to query for each example. Responses are then used
                    as the inputs as part of a local model and evaluated for their contribution. This cuts outward
                    bandwidth 5 by querying only a small subset of peers for each example and thus also reducing the
                    number of possible permutations in our Shapley calculation.</p>
                <p>The sparse combinations of peers effectively represent a subset of the possible permutations within
                    the complete set of N peers. Given enough samples, we can recover the full Shapley values and a
                    cooperative ranking r for the entire network. Benchmarks of several LLMs currently running on the
                    Atomic network are showcased in Figure 5.</p>
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">05 the current state of the Atomic network</p>
                <p>The Atomic network officially launched in November 2021. As of writing this paper, the network
                    contains 4096 peers; all of which are running a language model that is actively querying others for
                    information or serving requests from other peers on the network. With a total sum of 500 Billion
                    parameters running across 4096 peers, it is possible to query and infer from every node in the
                    network and receive an output of logits, embeddings, or plain text responses given a prompt. Each
                    peer is either running a custom language model, or a pre-trained model such as GPT-J (Wang and
                    Komatsuzaki [2021]), GPT-Neo, (Black et al. [2021]), or GPT-2 (Radford et al. [2019]) downloaded
                    from HuggingFace API (Wolf et al. [2020]).</p>
                <p>In 2022, the protocol underwent several algorithmic, consensus, and infrastructure changes to ensure
                    correctness and scalability. The Atomic team is now training more sophisticated models directly
                    on the Atomic protocol to work towards achieving performance comparable to current state of the
                    art models.</p>
            </section>
            <section class="page_section__q5fw_">
                <p class="page_subtitle__yxpgk">06 Conclusion</p>
                <p>We have introduced the first peer-to-peer, trust-less intelligence market that features a novel
                    ranking benchmark. The benchmark measures performance as representational-knowledge production using
                    other intelligence systems to determine its value. Due to its high-resolution, highly collaborative
                    nature, this benchmark has the potential to be the gold standard in machine learning testing and
                    evaluation.</p>
                <p>The paper began with the definition of the Atomic Protocol, a P2P network composed of abstractly
                    defined intelligence models. Peers are able to directly query each other for informational
                    embeddings that serve as inputs to their own models. Training can occur by subsequently propagating
                    backward gradients to responsive peers. We then showed how this framework allows us to produce a
                    cooperative game-theoretic ranking for each peer based on its informational value for the network.
                    These scores are passed on to a digital ledger where a trust-based incentivization is distributed.
                    Peers that earn high scores from a majority of stake holders will receive the majority of the
                    incentive. We then demonstrated how this incentive scheme based on peer connectivity prevents
                    participants from forming dishonest sub-graphs such that over time, dishonest sub-graphs decay into
                    irrelevance.</p>
                <p>The result of these various novel mechanisms is an intelligence market that rewards participants for
                    producing knowledge while enabling the sharing of this knowledge for new learners in the system.
                    Similar to financial and other Web3 markets, the market for intelligence actively encourages
                    innovations among its participants and rewards for those who manage to provide unique knowledge to
                    the collective. In terms of ML, Atomic provides incentives for participants to consistently
                    improve and finetune their models over the network. This combination of Web3 and ML introduces an
                    unique network where a decentralized global contribution of compute can be put towards the
                    advancement of machine intelligence.</p>
            </section><span class="page_paper_link__BeAmh"><a class="Link_link__hI8T_" target="_blank"
                    href="/pdfs/academia/NeurIPS_DAO_Workshop_2022_3_3.pdf">Follow this link for the original
                    version</a></span>
        </div><!--/$-->
        <footer class="Footer_footer__Pj4YH">
            <ul class="Footer_footer_modules__qKPPz">
                <!-- <li><span class="Footer_footer_moduleTitle__vKvHn">Resources</span>
                    <ul class="Footer_footer_links__rx6_i">
                        <li><a class="Link_link__hI8T_" target="_self" href="/scan">Scantensor</a></li>
                        <li><a class="Link_link__hI8T_" target="_blank" href="https://taostats.io/">Taostats</a></li>
                    </ul>
                </li> -->
                <li><span class="Footer_footer_moduleTitle__vKvHn">Community</span>
                    <ul class="Footer_footer_links__rx6_i">
                        <li><a class="Link_link__hI8T_" target="_blank" href="tglink">Telegram</a>
                        </li>
                        <li><a class="Link_link__hI8T_" target="_blank" href="https://github.com/opentensor">github</a>
                        </li>
                    </ul>
                </li>
            </ul>
        </footer>
    </main>
    <script src="/_next/static/chunks/webpack-b9b43169dd91f866.js" async=""></script>
    <script src="/_next/static/chunks/3132-b3315bd5e5687ef4.js" async=""></script>
    <script src="/_next/static/chunks/main-app-ae0b34b98fd93541.js" async=""></script>
</body>

</html>
<!-- <script>(self.__next_f = self.__next_f || []).push([0])</script>
<script>self.__next_f.push([1, "0:\"$L1\"\n"])</script>
<script>self.__next_f.push([1, "2:I{\"id\":\"97030\",\"chunks\":[\"2272:webpack-b9b43169dd91f866\",\"3132:3132-b3315bd5e5687ef4\"],\"name\":\"\",\"async\":false}\n4:I{\"id\":\"92738\",\"chunks\":[\"2272:webpack-b9b43169dd91f866\",\"3132:3132-b3315bd5e5687ef4\"],\"name\":\"\",\"async\":false}\n5:I{\"id\":\"3400\",\"chunks\":[\"4819:4819-dc0e949e1fc53eb6\",\"3185:app/layout-e539cd24ea266263\"],\"name\":\"default\",\"async\":true}\n6:I{\"id\":\"12158\",\"chunks\":[\"2272:webpack-b9b43169dd91f866\",\"3132:3132-b3315bd5e5687ef4\"],\"name\":\"\",\"async\":false}\n7:I{\"id\":\"52932\",\"chunks\":[\"2272:webpack-b9b4316"])</script>
<script>self.__next_f.push([1, "9dd91f866\",\"3132:3132-b3315bd5e5687ef4\"],\"name\":\"\",\"async\":false}\n"])</script>
<script>self.__next_f.push([1, "1:[\"$\",\"$L2\",null,{\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/academia\",\"initialTree\":[\"\",{\"children\":[\"(pages-with-footer)\",{\"children\":[\"academia\",{\"children\":[\"\",{}]}]}]},null,null,true],\"initialHead\":[\"$L3\",null],\"globalErrorComponent\":\"$4\",\"children\":[[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"hasLoading\":false,\"template\":[\"$\",\"$L7\",null,{}],\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"childProp\":{\"current\":[\"$L8\",null,null,[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/eee66c5eb8dff5dd.css\",\"precedence\":\"next.js\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/86333f61218991e5.css\",\"precedence\":\"next.js\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/08489188df8cd5b7.css\",\"precedence\":\"next.js\"}]]],\"segment\":\"(pages-with-footer)\"}}],\"params\":{}}],null,null,[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a8a68dea0a60e9b6.css\",\"precedence\":\"next.js\"}]]]}]\n"])</script>
<script>self.__next_f.push([1, "9:I{\"id\":\"43767\",\"chunks\":[\"9774:framework-6b596cd218968a7f\",\"8761:8761-48f228d0a8b3b0fb\",\"7183:7183-36e34390985288ae\",\"9507:9507-5b06b0feac3f68e9\",\"1410:1410-64d072bed1b93731\",\"8559:8559-303e4e1b61e03473\",\"3767:3767-7d8171d6b705db23\",\"3948:app/(pages-without-footer)/layout-08dd15cedc1455a4\"],\"name\":\"Header\",\"async\":false}\na:I{\"id\":\"77836\",\"chunks\":[\"9774:framework-6b596cd218968a7f\",\"8761:8761-48f228d0a8b3b0fb\",\"7183:7183-36e34390985288ae\",\"3119:3119-fef14e08ff03315a\",\"1410:1410-64d072bed1b93731\",\"8559:8559"])</script>
<script>self.__next_f.push([1, "-303e4e1b61e03473\",\"1105:app/(pages-with-footer)/academia/page-bdd24176f1ffa3ec\"],\"name\":\"default\",\"async\":true}\nb:I{\"id\":\"46389\",\"chunks\":[\"8076:5d96550a-8c7c76d711aa9ded\",\"8761:8761-48f228d0a8b3b0fb\",\"1677:1677-7c9d73643ca0a628\",\"9624:9624-511868d3b1403471\",\"4342:app/(pages-without-header)/scan/stakes/page-1676bad2a28a034d\"],\"name\":\"\",\"async\":false}\n"])</script>
<script>self.__next_f.push([1, "8:[[\"$\",\"$L9\",null,{}],[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(pages-with-footer)\",\"children\"],\"hasLoading\":false,\"template\":[\"$\",\"$L7\",null,{}],\"childProp\":{\"current\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(pages-with-footer)\",\"children\",\"academia\",\"children\"],\"hasLoading\":false,\"template\":[\"$\",\"$L7\",null,{}],\"childProp\":{\"current\":[[\"$\",\"$La\",null,{\"params\":{},\"searchParams\":{}}],null,null,[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/09ff7ae0c74d1ead.css\",\"precedence\":\"next.js\"}]]],\"segment\":\"\"}}],\"segment\":\"academia\"}}],[\"$\",\"footer\",null,{\"className\":\"Footer_footer__Pj4YH\",\"children\":[\"$\",\"ul\",null,{\"className\":\"Footer_footer_modules__qKPPz\",\"children\":[[\"$\",\"li\",\"footer-module-1\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"Footer_footer_moduleTitle__vKvHn\",\"children\":\"Resources\"}],[\"$\",\"ul\",null,{\"className\":\"Footer_footer_links__rx6_i\",\"children\":[[\"$\",\"li\",\"footer-module-1-link-1\",{\"children\":[\"$\",\"$Lb\",null,{\"href\":\"/scan\",\"className\":\"Link_link__hI8T_\",\"target\":\"_self\",\"scroll\":false,\"children\":\"Scantensor\"}]}],[\"$\",\"li\",\"footer-module-1-link-2\",{\"children\":[\"$\",\"$Lb\",null,{\"href\":\"https://taostats.io/\",\"className\":\"Link_link__hI8T_\",\"target\":\"_blank\",\"scroll\":false,\"children\":\"Taostats\"}]}]]}]]}],[\"$\",\"li\",\"footer-module-2\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"Footer_footer_moduleTitle__vKvHn\",\"children\":\"Community\"}],[\"$\",\"ul\",null,{\"className\":\"Footer_footer_links__rx6_i\",\"children\":[[\"$\",\"li\",\"footer-module-2-link-2\",{\"children\":[\"$\",\"$Lb\",null,{\"href\":\"tglink\",\"className\":\"Link_link__hI8T_\",\"target\":\"_blank\",\"scroll\":false,\"children\":\"Telegram\"}]}],[\"$\",\"li\",\"footer-module-2-link-3\",{\"children\":[\"$\",\"$Lb\",null,{\"href\":\"https://github.com/opentensor\",\"className\":\"Link_link__hI8T_\",\"target\":\"_blank\",\"scroll\":false,\"children\":\"github\"}]}]]}]]}]]}]}]]\n"])</script>
<script>self.__next_f.push([1, "3:[[[\"$\",\"meta\",null,{\"charSet\":\"utf-8\"}],null,null,null,null,null,null,null,null,null,null,[\"$\",\"meta\",null,{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],null,null,null,null,null,null,null,null,null,null,[]],[null,null,null,null],null,null,[null,null,null,null,null],null,null,null,null,[null,[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/_next/static/media/metadata/favicon.83662ded.ico\",\"type\":\"image/x-icon\",\"sizes\":\"any\"}]],[],null]]\n"])</script> -->